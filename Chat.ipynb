{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUM6AAt2VRD/U9F7s2MSBk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabihsabeh85/Chatbot-/blob/main/Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up Environment Variables"
      ],
      "metadata": {
        "id": "buAxzjhUHzJJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KVKVXwezHP74"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"\"\n",
        "os.environ[\"PINECONE_ENVIRONMENT\"] = \"\"\n",
        "os.environ[\"PINECONE_INDEX_NAME\"] = \"\"\n",
        "os.environ[\"PINECONE_KNOWLEDGE_NAMESPACE\"] = \"\"\n",
        "os.environ[\"PINECONE_CHAT_NAMESPACE\"] = \"\"\n",
        "os.environ[\"UPSTASH_REDIS_REST_URL\"] = \"\"\n",
        "os.environ[\"UPSTASH_REDIS_REST_TOKEN\"] = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing Required Packages\n",
        "!pip install aiohttp aiosignal annotated-types anyio attrs certifi charset-normalizer click colorama dataclasses-json distro dnspython email_validator fastapi fastapi-cli frozenlist greenlet h11 httpcore httptools httpx idna Jinja2 jsonpatch jsonpointer langchain langchain-community langchain-core langchain-openai langchain-pinecone langchain-text-splitters langsmith markdown-it-py MarkupSafe marshmallow mdurl multidict mypy-extensions numpy openai orjson packaging pinecone pinecone-client pydantic pydantic_core Pygments python-dotenv python-multipart PyYAML regex requests rich shellingham six sniffio SQLAlchemy starlette tenacity tiktoken tqdm typer typing-inspect typing_extensions ujson upstash-redis urllib3 uvicorn watchfiles websockets yarl\n"
      ],
      "metadata": {
        "id": "ijYbTD5FH4Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_assistant.py\n",
        "import urllib3\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "CONDENSE_TEMPLATE = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
        "<chat_history>\n",
        "  {chat_history}\n",
        "</chat_history>\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "\n",
        "QA_TEMPLATE = \"\"\"You are an expert report analyst. Use the following pieces of context to answer the question at the end.\n",
        "Current time: {current_time}.\n",
        "Please answer the question considering the context.\n",
        "<context>\n",
        "  {context}\n",
        "</context>\n",
        "<chat_history>\n",
        "  {chat_history}\n",
        "</chat_history>\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "class EducatorAssistant:\n",
        "    def __init__(self, pinecone_index):\n",
        "        self.model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
        "        self.index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
        "        self.embed_model = OpenAIEmbeddings(\n",
        "            model=\"text-embedding-ada-002\",\n",
        "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        "        )\n",
        "        self.history = []\n",
        "\n",
        "        self.pinecone_index = pinecone_index\n",
        "        self.prompt_qa = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", QA_TEMPLATE),\n",
        "        ])\n",
        "        self.prompt_condense = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", CONDENSE_TEMPLATE),\n",
        "        ])\n",
        "        self.chain_qa = self.prompt_qa | self.model\n",
        "        self.chain_condense = self.prompt_condense | self.model\n",
        "\n",
        "    def chat(self, user_input):\n",
        "        condense_question = self.chain_condense.invoke({\n",
        "            \"question\": user_input,\n",
        "            \"chat_history\": self.history\n",
        "        })\n",
        "\n",
        "        vectorstore_knowledge = PineconeVectorStore(\n",
        "            index_name=self.index_name, embedding=self.embed_model, namespace=os.getenv(\"PINECONE_KNOWLEDGE_NAMESPACE\")\n",
        "        )\n",
        "\n",
        "        query = condense_question.content\n",
        "\n",
        "        docs_knowledge = vectorstore_knowledge.similarity_search(query, k=20)\n",
        "        separator = '\\n\\n'\n",
        "        serialized_docs_knowledge = [doc.page_content for doc in docs_knowledge]\n",
        "        knowledge_context = separator.join(serialized_docs_knowledge)\n",
        "\n",
        "        separator = '\\n'\n",
        "        serialized_history = [f\"{key}: {value}\" for message in self.history[-5:] for key, value in message.items()]\n",
        "        history = separator.join(serialized_history)\n",
        "\n",
        "        current_datetime = datetime.datetime.now()\n",
        "        current_datetime_str = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        answer = self.chain_qa.invoke({\n",
        "            \"question\": user_input,\n",
        "            \"chat_history\": history,\n",
        "            \"context\": knowledge_context,\n",
        "            \"current_time\": current_datetime_str\n",
        "        })\n",
        "        self.history.append({\"Human\": user_input})\n",
        "        self.history.append({\"Assistant\": answer.content})\n",
        "        return answer.content\n"
      ],
      "metadata": {
        "id": "JG6WY7INH8LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_server.py\n",
        "from dotenv import load_dotenv\n",
        "from upstash_redis import Redis\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Query\n",
        "import os\n",
        "import uuid\n",
        "import json\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from pinecone import Pinecone\n",
        "from pydantic import BaseModel\n",
        "from test_assistant import EducatorAssistant\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "upstash_redis_url = os.getenv(\"UPSTASH_REDIS_REST_URL\")\n",
        "upstash_redis_token = os.getenv(\"UPSTASH_REDIS_REST_TOKEN\")\n",
        "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "redis = Redis(\n",
        "    url=upstash_redis_url,\n",
        "    token=upstash_redis_token\n",
        ")\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "index = pc.Index(pinecone_index_name)\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "origins = [\"*\"]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "sessions = {}\n",
        "from typing import Dict\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "client_last_activity: Dict[str, datetime] = {}\n",
        "\n",
        "def generate_unique_uuid():\n",
        "    return str(uuid.uuid1())\n",
        "\n",
        "def set_messages(user_id, messages):\n",
        "    res = redis.set(user_id, {\"data\": messages})\n",
        "    return res\n",
        "\n",
        "def get_messages(user_id):\n",
        "    messages = redis.get(user_id)\n",
        "    if messages is None:\n",
        "        return []\n",
        "    return json.loads(messages)[\"data\"]\n",
        "\n",
        "class MessageList(BaseModel):\n",
        "    session_id: str\n",
        "    message: str\n",
        "\n",
        "@app.post(\"/start\")\n",
        "async def say_hello():\n",
        "    session_id = generate_unique_uuid()\n",
        "    assistant = EducatorAssistant(index)\n",
        "    sessions[session_id] = assistant\n",
        "    return {\n",
        "        \"session_id\": session_id,\n",
        "        \"message\": \"Hello, How can I help you?\"\n",
        "    }\n",
        "\n",
        "@app.post(\"/end\")\n",
        "async def end(req: MessageList):\n",
        "    assistant = None\n",
        "    if req.session_id in sessions:\n",
        "        assistant = sessions[req.session_id]\n",
        "        result = set_messages(req.session_id, assistant.history)\n",
        "        sessions.pop(req.session_id)\n",
        "        if result:\n",
        "            return \"save success\"\n",
        "        else:\n",
        "            return \"save failed\"\n",
        "    else:\n",
        "        return \"not found\"\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_with_teacher_agent(req: MessageList):\n",
        "    assistant = None\n",
        "    if req.session_id in sessions:\n",
        "        assistant = sessions[req.session_id]\n",
        "        client_last_activity[req.session_id] = datetime.utcnow()\n",
        "    else:\n",
        "        assistant = EducatorAssistant(index)\n",
        "        sessions[req.session_id] = assistant\n",
        "    response = assistant.chat(req.message)\n",
        "    return {\"message\": response}\n",
        "\n",
        "async def cleanup_chat_histories():\n",
        "    current_time = datetime.utcnow()\n",
        "    for session_id, last_activity in list(client_last_activity.items()):\n",
        "        if current_time - last_activity > timedelta(seconds=1800):\n",
        "            message_list = MessageList(session_id=session_id, message='end')\n",
        "            await end(message_list)\n",
        "            del client_last_activity[session_id]\n",
        "\n",
        "async def repeat_cleanup_task(wait_time: int):\n",
        "    while True:\n",
        "        await cleanup_chat_histories()\n",
        "        await asyncio.sleep(wait_time)\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def on_startup():\n",
        "    asyncio.create_task(repeat_cleanup_task(1800))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"test_server:app\", host=\"0.0.0.0\", port=8001, reload=True)\n"
      ],
      "metadata": {
        "id": "-wD0Gh4XIACY"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}